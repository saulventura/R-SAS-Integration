---
title: "Zillow's Home Value Prediction"

knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))
 }
 )

output:
  rmdformats::material:
    highlight: kate
    self_contained: no
    code_folding: hide
    thumbnails: true
    gallery: true
    fig_width: 4
    fig_height: 4
    df_print: kable
    
---

```{r knitr_init, echo=FALSE, results="asis", cache=FALSE, warning = FALSE, message=FALSE}
library(knitr)
library(rmdformats)
library(data.table)
library(dplyr)
library(ggplot2)
library(stringr)
library(DT)
library(tidyr)
library(corrplot)
library(leaflet)
library(lubridate)
library(maps)

## Global options
options(max.print = "75")
opts_chunk$set(echo = FALSE,
	             cache = FALSE,
               prompt = FALSE,
               tidy = FALSE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
opts_knit$set(width = 75)
```


# Introduction

**Zillow** is an online real estate database company that was founded in 2006. Basically, its business model is to generate revenue by selling advertising on its web site. As does its competitors Trulia or Realtor, Zillow offers a **home value estimation** that home buyers and sellers can find useful. This service is considered a great tool but is not perfect, thus, Zillow launched a **competition on Kaggle**. 

```{r figure1, echo=FALSE, fig.cap="", out.width = '80%' , out.height= '80%',fig.align ='center' }
knitr::include_graphics("logo.jpg")
```

## Business Goal

The main objective of this project is to propose an algorithm to predict a log-error between the price estimated by Zillow and the real home price based on sales.
The formula established by Zillow is 

 $$   Log.error = log (Zillow.estimation) - log (Home.price) $$

##Predictive Model Cycle

For all stages of the model development, I will use R and SAS Enterprise Miner. The initial stages are to get a better understanding of the data as well as to prepare it for the modeling process. These initial stages are data collection, data exploration, and data preparation. 

```{r figure2, echo=FALSE, fig.cap="", out.width = '80%' , out.height= '80%',fig.align ="center" }
knitr::include_graphics("process.jpg")
```

##Technologies

During the completion of this project, we will use **R** to convey higher quality data to **SAS Enterprise Miner** that can handle more than one machine learning algorithm, also SAS will allow us to use **The model comparison node** in order to compare the performance of some models....and we can do the setup **visually**.
So, instead of use just one tool, we will take advantage of the best of both worlds.
```{r figure3, echo=FALSE, fig.cap="", out.width = '100%' , out.height= '80%',fig.align ="center" }
knitr::include_graphics("sasandr.jpg")
```

#Data Collection


The datasets provided by Zillow has been made public through the Kaggle website and even though the competition is closed for new participants, the data is still available. The datasets contain housing data from 2016 for three different counties in California: Los Angeles County, Ventura County and the Orange County.

```{r figure4, echo=FALSE, fig.cap="", out.width = '80%' , out.height= '80%',fig.align ="center" }
knitr::include_graphics("data.jpg")
```

```{r Collection, echo=TRUE,eval=TRUE , warning = FALSE, message=FALSE}

properties <- read.csv('C:/Saul/Portfolio/data/properties_2016.csv')
transactions <- read.csv('C:/Saul/Portfolio/data/train_2016_v2.csv')
sample_submission <- read.csv('C:/Saul/Portfolio/data/sample_submission.csv')


names(properties)
names(transactions)
names(sample_submission)


```
Let's review a small sample of the data.
```{r Collection1, echo=TRUE,eval=TRUE , warning = FALSE, message=FALSE}


datatable(properties[(1:20),], filter = 'top', options = list(
  pageLength = 5, scrollX = TRUE, scrollY = "200px", autoWidth = TRUE), caption = 'Table 1: Properties.')



```

# Data Exploration

## Histogram graphics

Reviewing some of the data, the **build year** feature has almost a standard distribution, also an important proportion of the houses were built several years ago. Regarding the rest of features such as the number of bedrooms, number of bathrooms, number and capacity of garages, tax delinquency, property taxes, etc. Most of them have normalized data as well. Furthermore, the distribution for the critical attribute called log-error, as this is the variable to predict, has a normalized data.

```{r Exploration, echo=TRUE,eval=TRUE , warning = FALSE, message=FALSE}


df <- data.frame(properties)
ggplot(df, aes(x = yearbuilt)) +
  geom_histogram(bins = 20, fill="darkgreen", alpha=1) +
  geom_vline(aes(xintercept=mean(yearbuilt)), ## straight line for the mean
             colour = "#ADFF2F", size=1.5, alpha=0.5) + 
  geom_vline(aes(xintercept=median(yearbuilt)), ## dashed line for the median
             colour = "#ADFF2F", linetype="dashed", size=1.5, alpha=0.5) +
  labs(x = "Years", y = "Frequency") +
  ggtitle("Histogram Home Aging") 



# Rename the variable names
# FunctionX(dataA) is the same as dataA %>% functionX

properties <- properties %>% rename(
  id_parcel = parcelid,
  build_year = yearbuilt,
  area_basement = basementsqft,
  area_patio = yardbuildingsqft17,
  area_shed = yardbuildingsqft26, 
  area_pool = poolsizesum,  
  area_lot = lotsizesquarefeet, 
  area_garage = garagetotalsqft,
  area_firstfloor_finished = finishedfloor1squarefeet,
  area_total_calc = calculatedfinishedsquarefeet,
  area_base = finishedsquarefeet6,
  area_live_finished = finishedsquarefeet12,
  area_liveperi_finished = finishedsquarefeet13,
  area_total_finished = finishedsquarefeet15,  
  area_unknown = finishedsquarefeet50,
  num_unit = unitcnt, 
  num_story = numberofstories,  
  num_room = roomcnt,
  num_bathroom = bathroomcnt,
  num_bedroom = bedroomcnt,
  num_bathroom_calc = calculatedbathnbr,
  num_bath = fullbathcnt,  
  num_75_bath = threequarterbathnbr, 
  num_fireplace = fireplacecnt,
  num_pool = poolcnt,  
  num_garage = garagecarcnt,  
  region_county = regionidcounty,
  region_city = regionidcity,
  region_zip = regionidzip,
  region_neighbor = regionidneighborhood,  
  tax_total = taxvaluedollarcnt,
  tax_building = structuretaxvaluedollarcnt,
  tax_land = landtaxvaluedollarcnt,
  tax_property = taxamount,
  tax_year = assessmentyear,
  tax_delinquency = taxdelinquencyflag,
  tax_delinquency_year = taxdelinquencyyear,
  zoning_property = propertyzoningdesc,
  zoning_landuse = propertylandusetypeid,
  zoning_landuse_county = propertycountylandusecode,
  flag_fireplace = fireplaceflag, 
  flag_tub = hashottuborspa,
  quality = buildingqualitytypeid,
  framing = buildingclasstypeid,
  material = typeconstructiontypeid,
  deck = decktypeid,
  story = storytypeid,
  heating = heatingorsystemtypeid,
  aircon = airconditioningtypeid,
  architectural_style= architecturalstyletypeid
)

transactions <- transactions %>% rename(
  id_parcel = parcelid,
  date = transactiondate
)

# Convert dummary variables (Y and N) to (1 and 0)
properties <- properties %>% 
  mutate(tax_delinquency = ifelse(tax_delinquency=="Y",1,0),
         flag_fireplace = ifelse(flag_fireplace=="Y",1,0),
         flag_tub = ifelse(flag_tub=="Y",1,0))

# Take a look at the data
properties <- properties %>% select(id_parcel, build_year, starts_with("area_"), 
                                    starts_with("num_"), starts_with("flag_"), starts_with("region_"), everything())


#datatable(head(properties,100), style="bootstrap", class="table-condensed", options = list(dom = 'tp',scrollX = TRUE))
#datatable(head(transactions,100), style="bootstrap", class="table-condensed", options = list(dom = 'tp'))

```

Let's take a look at the transaction data.

```{r Exploration2, echo=TRUE,eval=TRUE , warning = FALSE, message=FALSE}



tmp <- transactions %>% mutate(year_month = make_date(year=year(date),month=month(date)))
tmp %>% 
  group_by(year_month) %>% count() %>% 
  ggplot(aes(x=year_month,y=n)) +
  geom_bar(stat="identity", fill="darkgreen")+
  geom_vline(aes(xintercept=as.numeric(as.Date("2016-10-01"))),size=2)

```



```{r Exploration3, echo=TRUE,eval=TRUE , warning = FALSE, message=FALSE}





```

## Log data plots {.tabset .tabset-fade .tabset-pills}


Let's review the distribution of Zestimate's forecast errors (log error).

Also, let's check how is the distribution when using an absolute logerror.

Then, let's see how does log error change along the time.


### Log Error Histogram

```{r Exploration4, echo=TRUE,eval=TRUE , warning = FALSE, message=FALSE}


transactions %>% 
  ggplot(aes(x=logerror)) + 
  geom_histogram(bins=400, fill="darkgreen")+
  theme_bw()+theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+
  ylab("Frequency")+coord_cartesian(x=c(-0.5,0.5)) +
  ggtitle("Histogram Log error") 

```

### Absolute Log Error Histogram

```{r Exploration5, echo=TRUE,eval=TRUE , warning = FALSE, message=FALSE}



# Absolute logerror
transactions <- transactions %>% mutate(abs_logerror = abs(logerror))
transactions %>% 
  ggplot(aes(x=abs_logerror)) + 
  geom_histogram(bins=400, fill="#0da107")+
  theme_bw()+theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+
  ylab("Frequency")+coord_cartesian(x=c(0,0.5))

```

### Log Error through the time

```{r Exploration6, echo=TRUE,eval=TRUE , warning = FALSE, message=FALSE}

# How does log error change with time
transactions %>% 
  mutate(year_month = make_date(year=year(date),month=month(date)) ) %>% 
  group_by(year_month) %>% summarize(mean_logerror = mean(logerror)) %>% 
  ggplot(aes(x=year_month,y=mean_logerror)) + 
  geom_line(size=1.5, color="darkgreen")+geom_point(size=5, color="lightgreen")+theme_bw()


```


# Data Preparation

## Data Cleaning

Let's plot a missingness chart to see that there are several features which have a bunch of missing values. 

```{r Preparation, echo=TRUE,eval=TRUE , fig.width=7, fig.height=8, warning = FALSE, message=FALSE}

missing_values <- properties %>% summarize_all(funs(sum(is.na(.))/n()))

missing_values <- gather(missing_values, key="feature", value="missing_pct")
missing_values %>% 
  ggplot(aes(x=reorder(feature,-missing_pct),y=missing_pct)) +
  geom_bar(stat="identity",fill="#0da107")+
  coord_flip()+theme_bw()+
  labs(y = "Missingness", x = "Features") +
  ggtitle("Missing data") 

```
Based on data provided by the above chart, We will retrieve only the features whose missing data is not more than 25%. There are 39 attributes, as shown in below table.

```{r Preparation1, echo=TRUE,eval=TRUE , fig.width=7, fig.height=8, warning = FALSE, message=FALSE}

good_features <- filter(missing_values, missing_pct<0.25)
good_features

```
## Correlation Analysis

Next step is to review the correlation among the features. After using sets of attributes we can note that some of them are highly correlated, such as the number of calculated bathrooms and the actual number of bathrooms, similarly, there is a correlation for the total tax and the property tax. Correlated features had to be removed.

Let's see correlation among room features.

```{r Preparation2, echo=TRUE,eval=TRUE , fig.width=7, fig.height=7, warning = FALSE, message=FALSE}


col3 <- colorRampPalette(c("blue", "white", "#4bb048")) 

  vars <- good_features$feature[str_detect(good_features$feature,'num_')]
cor_tmp <- transactions %>% left_join(properties, by="id_parcel") 
tmp <- cor_tmp %>% select(one_of(c(vars,"logerror")))
corrplot(cor(tmp, use="complete.obs"),type="lower", col = col3(20))


```

Let's see correlation among tax amount features.
```{r Preparation3, echo=TRUE,eval=TRUE , fig.width=7, fig.height=7,   warning = FALSE, message=FALSE}

vars <- setdiff(good_features$feature[str_detect(good_features$feature,'tax_')],c("tax_delinquency","tax_year"))
tmp <- cor_tmp %>%  select(one_of(c(vars,"logerror")))
corrplot(cor(tmp, use="complete.obs"), type="lower", col = col3(20))

```



## Classical tabs {.tabset}

classical tabs without fading effect.

### First tab

Work in progress...

### Second tab

Work in progress...




# Modeling
Work in progress...

#Validation
Work in progress...

#Results
Work in progress...
